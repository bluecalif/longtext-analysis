---
alwaysApply: true
description: 테스트 전략 및 E2E 테스트 규칙
---

# Testing Strategy

## Overview
프로젝트는 E2E 테스트 중심의 테스트 전략을 사용합니다. Phase별로 적절한 테스트 방식을 선택하여 프로덕션 플로우와 동일하게 검증합니다.

## Domain Knowledge

### 테스트 원칙
1. **E2E 테스트 우선**: 모든 테스트는 실제 데이터 사용 필수
2. **실제 데이터 사용**: Mock 사용 절대 금지
3. **프로덕션 플로우 동일**: 배포 후 문제가 없음을 보장
4. **자동 보고 필수**: 테스트 실행 후 상세 분석 리포트
5. **로그 파일 저장 필수**: 모든 E2E 테스트는 로그 파일 저장 (디버깅 필수)
6. **실행 결과 저장 필수**: 파싱/추출 결과를 상세히 저장 (사용자 리뷰/피드백 필수)

### Phase별 서버 실행 구분
- **Phase 2-5**: 모듈 직접 호출 (서버 불필요)
  - 파서, 이벤트 정규화, Timeline/Issues 빌더 등은 모듈 직접 호출
  - 서버 시작 오버헤드 없이 빠른 테스트 가능
- **Phase 6+**: 실제 서버 실행 (API 테스트 필수)
  - API 엔드포인트 테스트는 실제 uvicorn 서버 실행 필수
  - HTTP 레벨 통합 검증 필요

### 테스트 입력 데이터
- 실제 Cursor export 파일: `docs/cursor_phase_6_3.md`
- 각 Phase별 E2E 테스트에서 동일 파일 사용

### 테스트 단계별 검증
- **정합성 검증**: 구조적 정확성 확인
- **타당성 검증**: 실제 상황에서의 품질 확인
- **결과 분석**: 상세 분석 및 개선점 도출

## Standards & Conventions

### 1. 파일 구조
```
tests/
├── fixtures/
│   └── cursor_phase_6_3.md    # 실제 입력 데이터
├── golden/
│   ├── parser_expected.json
│   ├── timeline_expected.json
│   └── issues_expected.json
├── logs/                        # 로그 파일 (자동 생성)
│   ├── test_parser_e2e_20250101_120000.log
│   └── ...
├── results/                     # 실행 결과 (자동 생성)
│   ├── parser_e2e_20250101_120000.json
│   └── ...
├── reports/                     # 리포트 파일 (자동 생성)
│   ├── parser_e2e_report.json
│   └── ...
├── conftest_e2e.py             # E2E 테스트 fixture
├── test_parser_e2e.py
├── test_event_normalizer_e2e.py
├── test_timeline_issues_e2e.py
├── test_snippet_e2e.py
└── test_api_e2e.py
```

### 2. E2E 테스트 Fixture

**⚠️ 중요**: pytest는 `conftest.py` 파일만 자동으로 인식합니다. `conftest_e2e.py` 같은 다른 이름은 자동 로드되지 않아 fixture가 실행되지 않을 수 있습니다.

**해결 방법**: 
- `conftest.py` 파일에 fixture를 정의하거나
- `conftest_e2e.py`의 내용을 `conftest.py`로 병합

**⚠️ 중요: 서버 관리 규칙**:

1. **포트 충돌 방지 (필수)**
   - 서버 시작 전에 포트 8000이 사용 중인지 확인
   - 포트가 사용 중인 경우 기존 프로세스를 자동으로 종료
   - psutil이 있으면 psutil 사용, 없으면 Windows에서는 netstat/taskkill 사용
   - 이 규칙을 준수하지 않으면 테스트 실행 시 포트 충돌로 인한 오류 발생 가능

2. **서버 로그 파일 저장 (필수)**
   - 서버 stdout/stderr를 파일로 리다이렉트하여 저장
   - 로그 파일 위치: `tests/logs/server_{timestamp}.log`
   - 디버깅 및 캐시 사용 여부 확인 목적
   - **FastAPI 앱 로깅 설정**: `backend/main.py`에 `logging.basicConfig()` 추가 (INFO 레벨)
   - **LLM 서비스 로그**: CACHE HIT/MISS 로그는 INFO 레벨로 출력 (서버 로그에 포함)
   - **API 엔드포인트 로깅**: 각 라우터에서 `use_llm` 파라미터와 결과 로깅

3. **서버 종료 보장 (필수)**
   - 테스트 완료 후 반드시 서버 프로세스 종료
   - finally 블록에서 서버 종료 및 로그 파일 닫기 처리

```python
# tests/conftest.py
import pytest
import subprocess
import time
import httpx
import logging
import socket
import platform
from pathlib import Path
from datetime import datetime

# psutil은 선택적 의존성 (없어도 동작하되 포트 충돌 방지 기능 제한)
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

# Phase 6+ 전용: 실제 서버 실행 fixture
@pytest.fixture(scope="session")
def test_server():
    """
    실제 uvicorn 서버 실행 fixture (Phase 6+ API 테스트용)
    
    서버 로그를 파일로 저장합니다 (캐싱 사용 여부 확인 및 디버깅 목적).
    
    기존 서버가 실행 중인 경우 포트 충돌을 방지하기 위해 자동으로 종료합니다.
    
    Phase 2-5에서는 사용하지 않음 (모듈 직접 호출)
    """
    # 포트 8000 사용 중인 프로세스 확인 및 종료
    port = 8000
    logger = logging.getLogger(__name__)
    
    # 포트가 사용 중인지 확인
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(0.1)
    result = sock.connect_ex(('localhost', port))
    sock.close()
    
    if result == 0:
        # 포트가 사용 중인 경우
        logger.warning(f"Port {port} is already in use. Attempting to kill existing process...")
        
        if HAS_PSUTIL:
            # psutil을 사용하여 포트를 사용하는 프로세스 찾기 및 종료
            for proc in psutil.process_iter(['pid', 'name', 'connections']):
                try:
                    connections = proc.info.get('connections')
                    if connections:
                        for conn in connections:
                            if conn.status == 'LISTEN' and conn.laddr.port == port:
                                logger.warning(f"Killing process {proc.info['pid']} ({proc.info['name']}) using port {port}")
                                proc.terminate()
                                proc.wait(timeout=5)
                                logger.info(f"Process {proc.info['pid']} terminated successfully")
                                break
                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                    continue
            time.sleep(1)
        elif platform.system() == "Windows":
            # Windows에서 netstat과 taskkill 사용 (psutil 없이도 동작)
            try:
                netstat_result = subprocess.run(
                    ["netstat", "-ano"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                for line in netstat_result.stdout.splitlines():
                    if f":{port}" in line and "LISTENING" in line:
                        parts = line.split()
                        if len(parts) >= 5:
                            pid = parts[-1]
                            try:
                                logger.warning(f"Killing process {pid} using port {port}")
                                subprocess.run(
                                    ["taskkill", "/F", "/PID", pid],
                                    capture_output=True,
                                    timeout=5
                                )
                                logger.info(f"Process {pid} terminated successfully")
                            except subprocess.TimeoutExpired:
                                logger.warning(f"Failed to kill process {pid}: timeout")
                time.sleep(1)
            except Exception as e:
                logger.warning(f"Failed to kill process using netstat/taskkill: {e}")
        else:
            logger.warning("psutil not available. Cannot automatically kill existing process on port 8000")
    
    # 서버 로그 파일 경로 생성
    log_dir = Path(__file__).parent / "logs"
    log_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    server_log_file = log_dir / f"server_{timestamp}.log"
    
    # 서버 시작 (stdout/stderr를 파일로 리다이렉트)
    log_file = open(server_log_file, "w", encoding="utf-8")
    try:
        process = subprocess.Popen(
            ["poetry", "run", "uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"],
            cwd=Path(__file__).parent.parent,
            stdout=log_file,
            stderr=subprocess.STDOUT,  # stderr도 stdout으로 합쳐서 같은 파일에 저장
        )
        
        # 서버 준비 대기
        max_retries = 30
        for _ in range(max_retries):
            try:
                response = httpx.get("http://localhost:8000/health", timeout=1.0)
                if response.status_code == 200:
                    break
            except:
                time.sleep(0.5)
        else:
            process.terminate()
            log_file.flush()
            log_file.close()
            raise RuntimeError("Server failed to start")
        
        yield process
        
        # 서버 종료
        process.terminate()
        process.wait()
        log_file.flush()
    finally:
        # 서버 로그 파일 닫기
        log_file.close()
        print(f"\n[LOG] Server log saved to: {server_log_file.absolute()}")

@pytest.fixture
def client(test_server):
    """
    httpx.Client fixture (실제 HTTP 요청, Phase 6+ 전용)
    
    Phase 2-5에서는 사용하지 않음 (모듈 직접 호출)
    """
    return httpx.Client(base_url="http://localhost:8000", timeout=30.0)

# 모든 E2E 테스트에 자동 적용: 로그 파일 저장 (필수)
@pytest.fixture(autouse=True)
def setup_test_logging(request):
    """
    모든 E2E 테스트에 자동으로 로깅 설정 (필수)
    
    로그 파일 위치: tests/logs/{test_name}_{timestamp}.log
    
    ⚠️ 중요 사항:
    - 파일명은 반드시 `conftest.py`여야 함 (pytest 자동 인식)
    - `autouse=True`로 설정하여 모든 테스트에 자동 적용
    - FileHandler를 명시적으로 flush하여 로그 파일 즉시 저장
    """
    log_dir = None
    log_file = None
    file_handler = None
    
    try:
        # 1. 로그 디렉토리 생성
        log_dir = Path(__file__).parent / "logs"
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        test_name = request.node.name
        log_file = log_dir / f"{test_name}_{timestamp}.log"
        
        # 디버깅: 시작 메시지
        print(f"\n[DEBUG] Setting up logging for test: {test_name}")
        print(f"[DEBUG] Log directory: {log_dir.absolute()}")
        print(f"[DEBUG] Log file: {log_file.absolute()}")
        
        # 2. 기존 핸들러 제거 (중복 방지)
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
            handler.close()  # 핸들러 닫기
        
        # 3. FileHandler 생성 (버퍼링 비활성화 또는 즉시 flush)
        file_handler = logging.FileHandler(log_file, encoding="utf-8", mode="w")
        file_handler.setLevel(logging.DEBUG)
        
        # StreamHandler 생성
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.DEBUG)
        
        # 포맷터 설정
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        file_handler.setFormatter(formatter)
        stream_handler.setFormatter(formatter)
        
        # 4. 루트 로거 설정
        logging.root.setLevel(logging.DEBUG)
        logging.root.addHandler(file_handler)
        logging.root.addHandler(stream_handler)
        
        # 5. backend.core.llm_service 로거 명시적으로 DEBUG 레벨로 설정
        llm_logger = logging.getLogger("backend.core.llm_service")
        llm_logger.setLevel(logging.DEBUG)
        llm_logger.propagate = True  # 루트 로거로 전파
        
        # 6. Fixture 로거로 테스트 시작 로그
        logger = logging.getLogger(__name__)
        logger.info(f"Test started: {test_name}")
        logger.info(f"Log file: {log_file.absolute()}")
        
        # 즉시 flush하여 파일에 쓰기 확인
        file_handler.flush()
        
        # 7. 파일 생성 확인
        if log_file.exists():
            print(f"[DEBUG] Log file created: {log_file.absolute()}")
            print(f"[DEBUG] Log file size: {log_file.stat().st_size} bytes")
        else:
            print(f"[ERROR] Log file was not created: {log_file.absolute()}")
        
        # 8. 핸들러 확인
        root_handlers = logging.root.handlers
        print(f"[DEBUG] Root logger handlers: {len(root_handlers)}")
        for i, handler in enumerate(root_handlers):
            print(f"[DEBUG] Handler {i}: {type(handler).__name__}")
        
        yield
        
        # 9. 테스트 종료 후 처리
        logger.info(f"Test completed: {test_name}")
        
        # 10. FileHandler 명시적으로 flush 및 닫기
        if file_handler:
            file_handler.flush()
            # 파일 핸들러는 루트 로거에서 제거하지 않고 닫기만 함
            # (다른 핸들러가 사용 중일 수 있음)
        
        # 11. 최종 파일 확인
        if log_file and log_file.exists():
            file_size = log_file.stat().st_size
            print(f"\n[LOG] Test log saved to: {log_file.absolute()}")
            print(f"[LOG] Log file size: {file_size} bytes")
            
            if file_size == 0:
                print(f"[WARNING] Log file is empty!")
            else:
                # 파일 내용 일부 확인 (처음 500자)
                try:
                    with open(log_file, "r", encoding="utf-8") as f:
                        content_preview = f.read(500)
                        print(f"[LOG] Log file preview (first 500 chars):")
                        print(content_preview)
                except Exception as e:
                    print(f"[ERROR] Failed to read log file: {e}")
        else:
            print(
                f"\n[ERROR] Log file was not created: {log_file.absolute() if log_file else 'Unknown'}"
            )
    
    except Exception as e:
        # 예외 발생 시 상세 정보 출력
        print(f"\n[ERROR] Failed to setup logging: {e}")
        import traceback
        traceback.print_exc()
        
        # 가능한 경우 로그 파일 경로 출력
        if log_file:
            print(f"[ERROR] Intended log file: {log_file.absolute()}")
        
        # 예외를 다시 발생시켜 테스트 실패로 표시
        raise
    
    finally:
        # 12. 정리 작업 (필요한 경우)
        # FileHandler는 루트 로거에 남겨두고, 다음 테스트에서 제거됨
        pass
```

### 3. E2E 테스트 패턴

```python
# tests/test_parser_e2e.py
import pytest
from pathlib import Path
from backend.parser import parse_markdown

def test_parser_e2e():
    """
    실제 입력 데이터로 전체 파서 파이프라인 테스트
    """
    # 1. 실제 입력 데이터 로드
    input_file = Path(__file__).parent.parent / "docs" / "cursor_phase_6_3.md"
    text = input_file.read_text(encoding='utf-8')
    
    # 2. 파싱 실행
    result = parse_markdown(text, source_doc=str(input_file))
    
    # 3. 정합성 검증
    assert result["session_meta"] is not None
    assert result["session_meta"].session_id is not None
    assert len(result["turns"]) > 0
    
    # Session Meta 추출 정확성
    assert result["session_meta"].phase is not None or result["session_meta"].phase is None
    # (phase가 없을 수도 있으므로 None도 허용)
    
    # Turn 블록 분할 정확성
    user_turns = [t for t in result["turns"] if t.speaker == "User"]
    cursor_turns = [t for t in result["turns"] if t.speaker == "Cursor"]
    unknown_turns = [t for t in result["turns"] if t.speaker == "Unknown"]
    
    assert len(user_turns) > 0, "User turns should exist"
    assert len(cursor_turns) > 0, "Cursor turns should exist"
    
    # Unknown 비율 검증
    unknown_ratio = len(unknown_turns) / len(result["turns"])
    assert unknown_ratio < 0.2, f"Unknown speaker ratio too high: {unknown_ratio:.1%}"
    
    # 코드 스니펫 추출 정확성
    all_code_blocks = []
    for turn in result["turns"]:
        all_code_blocks.extend(turn.code_blocks)
    
    assert len(all_code_blocks) > 0, "Code blocks should be extracted"
    
    # 4. 타당성 검증
    # 파싱 실패율 검증
    assert unknown_ratio < 0.2, "Parse failure rate too high"
    
    # 5. 결과 분석 및 피드백
    print(f"\n[Parser E2E Test Results]")
    print(f"Total turns: {len(result['turns'])}")
    print(f"User turns: {len(user_turns)}")
    print(f"Cursor turns: {len(cursor_turns)}")
    print(f"Unknown turns: {len(unknown_turns)} ({unknown_ratio:.1%})")
    print(f"Code blocks: {len(all_code_blocks)}")
    print(f"Artifacts: {len(result.get('artifacts', []))}")
```

### 4. API E2E 테스트 패턴

```python
# tests/test_api_e2e.py
import pytest
from pathlib import Path
import httpx

def test_parse_endpoint_e2e(client: httpx.Client):
    """
    POST /api/parse 엔드포인트 E2E 테스트
    """
    # 실제 입력 파일
    input_file = Path(__file__).parent.parent / "docs" / "cursor_phase_6_3.md"
    
    with open(input_file, "rb") as f:
        files = {"file": ("cursor_phase_6_3.md", f, "text/markdown")}
        response = client.post("/api/parse", files=files)
    
    assert response.status_code == 200
    
    data = response.json()
    
    # 응답 스키마 검증
    assert "session_meta" in data
    assert "turns" in data
    assert "events" in data
    
    # 파싱 결과 정확성
    assert data["session_meta"]["session_id"] is not None
    assert len(data["turns"]) > 0

def test_timeline_endpoint_e2e(client: httpx.Client):
    """
    POST /api/timeline 엔드포인트 E2E 테스트
    """
    # 1. 먼저 파싱
    input_file = Path(__file__).parent.parent / "docs" / "cursor_phase_6_3.md"
    
    with open(input_file, "rb") as f:
        files = {"file": ("cursor_phase_6_3.md", f, "text/markdown")}
        parse_response = client.post("/api/parse", files=files)
    
    parse_data = parse_response.json()
    
    # 2. Timeline 생성
    timeline_response = client.post(
        "/api/timeline",
        json={
            "session_id": parse_data["session_meta"]["session_id"],
            "events": parse_data["events"],
            "session_meta": parse_data["session_meta"]
        }
    )
    
    assert timeline_response.status_code == 200
    
    timeline_data = timeline_response.json()
    
    # Timeline JSON 구조 검증
    assert "session_meta" in timeline_data
    assert "timeline" in timeline_data
    assert len(timeline_data["timeline"]) > 0
    
    # Artifact/Snippet 연결 확인
    for event in timeline_data["timeline"]:
        assert "seq" in event
        assert "type" in event
        assert "summary" in event
```

### 5. 테스트 후 자동 보고

```python
# tests/test_parser_e2e.py
def test_parser_e2e_with_report():
    """
    테스트 실행 후 상세 보고 포함
    """
    # ... 테스트 실행 ...
    
    # 결과 분석
    report = {
        "test_name": "parser_e2e",
        "input_file": str(input_file),
        "results": {
            "total_turns": len(result["turns"]),
            "user_turns": len(user_turns),
            "cursor_turns": len(cursor_turns),
            "unknown_ratio": unknown_ratio,
            "code_blocks": len(all_code_blocks),
            "artifacts": len(result.get("artifacts", []))
        },
        "warnings": [],
        "errors": []
    }
    
    if unknown_ratio > 0.2:
        report["warnings"].append(
            f"High Unknown speaker ratio: {unknown_ratio:.1%}"
        )
    
    # 리포트 출력
    import json
    print("\n" + "="*50)
    print("E2E Test Report")
    print("="*50)
    print(json.dumps(report, indent=2, ensure_ascii=False))
    print("="*50)
    
    # 리포트 파일 저장
    report_file = Path(__file__).parent / "reports" / "parser_e2e_report.json"
    report_file.parent.mkdir(exist_ok=True)
    report_file.write_text(
        json.dumps(report, indent=2, ensure_ascii=False),
        encoding='utf-8'
    )
```

## Implementation Patterns

### Pattern 1: Golden 파일 비교

```python
# tests/test_parser_e2e.py
def test_parser_golden_comparison():
    """
    Golden 파일과 비교하여 회귀 테스트
    """
    # 실제 파싱
    result = parse_markdown(text, source_doc)
    
    # Golden 파일 로드
    golden_file = Path(__file__).parent / "golden" / "parser_expected.json"
    if golden_file.exists():
        expected = json.loads(golden_file.read_text(encoding='utf-8'))
        
        # 비교 (주요 필드만)
        assert result["session_meta"].session_id == expected["session_meta"]["session_id"]
        assert len(result["turns"]) == len(expected["turns"])
    else:
        # Golden 파일 생성 (최초 1회)
        golden_file.parent.mkdir(exist_ok=True)
        golden_file.write_text(
            json.dumps({
                "session_meta": result["session_meta"].dict(),
                "turns_count": len(result["turns"])
            }, indent=2, ensure_ascii=False),
            encoding='utf-8'
        )
```

### Pattern 2: 로그 파일 저장 (필수)

**⚠️ 중요**: 모든 E2E 테스트는 자동으로 로그 파일을 저장합니다. `conftest_e2e.py`의 `setup_test_logging` fixture가 자동 적용됩니다.

로그 파일 위치: `tests/logs/{test_name}_{timestamp}.log`

### Pattern 3: 실행 결과 저장 (필수)

**⚠️ 중요**: 모든 E2E 테스트는 상세한 실행 결과를 저장해야 합니다.

```python
# tests/test_parser_e2e.py
from datetime import datetime
from pathlib import Path
import json

def test_parser_e2e():
    # ... 테스트 실행 ...
    
    # 상세 결과 저장 (필수)
    results_dir = Path(__file__).parent / "results"
    results_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = results_dir / f"parser_e2e_{timestamp}.json"
    
    # Pydantic 모델을 dict로 변환하여 저장
    detailed_results = {
        "session_meta": result["session_meta"].dict(),
        "turns": [turn.dict() for turn in result["turns"]],
        "code_blocks": [block.dict() for block in result["code_blocks"]],
        "artifacts": result["artifacts"],
        "test_metadata": {
            "timestamp": timestamp,
            "input_file": str(INPUT_FILE),
            "test_name": "parser_e2e",
        }
    }
    
    results_file.write_text(
        json.dumps(detailed_results, indent=2, ensure_ascii=False),
        encoding="utf-8"
    )
    
    print(f"\n[RESULTS] Detailed results saved to: {results_file}")
```

**저장 위치**: `tests/results/{test_name}_{timestamp}.json`

## Examples

### 완전한 E2E 테스트 예시

```python
# tests/test_api_e2e.py
import pytest
from pathlib import Path
import httpx
import json

@pytest.mark.e2e
def test_full_pipeline_e2e(client: httpx.Client):
    """
    전체 파이프라인 E2E 테스트
    파일 업로드 → 파싱 → Timeline 생성 → Issues 생성 → Export
    """
    input_file = Path(__file__).parent.parent / "docs" / "cursor_phase_6_3.md"
    
    # 1. 파일 업로드 및 파싱
    with open(input_file, "rb") as f:
        files = {"file": ("cursor_phase_6_3.md", f, "text/markdown")}
        parse_response = client.post("/api/parse", files=files)
    
    assert parse_response.status_code == 200
    parse_data = parse_response.json()
    
    # 2. Timeline 생성
    timeline_response = client.post(
        "/api/timeline",
        json={
            "session_id": parse_data["session_meta"]["session_id"],
            "events": parse_data["events"],
            "session_meta": parse_data["session_meta"]
        }
    )
    assert timeline_response.status_code == 200
    timeline_data = timeline_response.json()
    
    # 3. Issues 생성
    issues_response = client.post(
        "/api/issues",
        json={
            "session_id": parse_data["session_meta"]["session_id"],
            "events": parse_data["events"],
            "turns": parse_data["turns"]
        }
    )
    assert issues_response.status_code == 200
    issues_data = issues_response.json()
    
    # 4. Export 테스트
    export_response = client.post(
        "/api/export/all",
        json={
            "session_id": parse_data["session_meta"]["session_id"]
        }
    )
    assert export_response.status_code == 200
    
    # 5. 결과 검증
    assert len(timeline_data["timeline"]) > 0
    assert len(issues_data["issues"]) >= 0  # 이슈가 없을 수도 있음
    
    # 6. 리포트 생성
    report = {
        "test": "full_pipeline_e2e",
        "timeline_events": len(timeline_data["timeline"]),
        "issues": len(issues_data["issues"]),
        "status": "PASS"
    }
    
    print(json.dumps(report, indent=2, ensure_ascii=False))
```

## Checklist

### E2E 테스트 작성 시
- [ ] Phase별 적절한 테스트 방식 선택
  - Phase 2-5: 모듈 직접 호출 (서버 불필요)
  - Phase 6+: 실제 서버 실행 fixture 사용 (`test_server`)
- [ ] Phase 6+에서만 `httpx.Client` 사용 (TestClient 금지)
- [ ] 실제 입력 데이터 사용 (Mock 사용 절대 금지)
- [ ] 정합성 검증 구현
- [ ] 타당성 검증 구현
- [ ] 결과 분석 및 리포트 생성
- [ ] 로그 파일 저장 (자동 적용, `setup_test_logging` fixture)
- [ ] 실행 결과 저장 (필수, `tests/results/` 디렉토리)

### ⚠️ 테스트 실행 검증 (필수)
**절대 금지**: 테스트 파일만 작성하고 실행하지 않은 채 완료 보고

- [ ] **테스트 실제 실행**: `poetry run pytest tests/test_xxx.py -v` (반드시 실행)
- [ ] **테스트 통과 확인**: 모든 테스트가 PASS 상태인지 확인
- [ ] **결과 파일 확인**: `tests/logs/`, `tests/results/` 디렉토리에 파일 생성 확인
- [ ] **완료 보고에 포함**: 테스트 실행 명령어 및 결과 상태 명시

### 테스트 실행 후
- [ ] 자동으로 상세 보고 출력
- [ ] 리포트 파일 저장 (`tests/reports/`)
- [ ] 실행 결과 저장 (`tests/results/`, 상세 파싱/추출 결과)
- [ ] 로그 파일 저장 (`tests/logs/`, 자동 적용)
- [ ] 문제 발견 시 원인 분석 포함
- [ ] 정량적 데이터 제시

## References
- AGENTS.md: 테스트 관련 내용
- E2E 테스트 입력: `docs/cursor_phase_6_3.md`
- Backend API: `.cursor/rules/backend-api-design.mdc`
