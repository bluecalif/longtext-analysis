---
alwaysApply: true
description: LLM 서비스 사용 규칙 및 네트워크 오류 처리
---

# LLM Service

## Overview
OpenAI API를 사용한 LLM 서비스 구현 규칙. 네트워크 오류, 타임아웃, Rate limit 등에 대한 견고한 처리와 캐싱을 통한 비용 절감을 보장합니다.

## Domain Knowledge

### LLM 사용 목적
- 이벤트 타입 분류 (status_review, plan, artifact, debug, completion, next_step, turn)
- 텍스트 요약 생성 (1-2문장)
- 모델: `gpt-4.1-mini` (사용자 지시대로 정확히 사용)

### 비용 분석
- 입력: $0.40 per 1M tokens
- 출력: $1.60 per 1M tokens
- 평균 500자 텍스트 → 약 125 tokens
- 평균 응답 (타입 + 요약) → 약 30 tokens
- 비용: (125 * 0.40 + 30 * 1.60) / 1M = $0.000098 per Turn
- 캐싱 적용 시: 70-80% 절감 → $0.0000196-0.0000294 per Turn

### 발생 가능한 문제
1. **Connection Failure**: 네트워크 지연, 타임아웃, 일시적 오류
2. **Rate Limit**: OpenAI API 요청 제한 초과
3. **타임아웃**: 응답 지연으로 인한 무한 대기
4. **캐싱 미작동**: 캐시 파일 저장/조회 실패

## Standards & Conventions

### 1. 필수 설정

**타임아웃 설정 (필수)**:
```python
# backend/core/llm_service.py
LLM_TIMEOUT = 120  # OpenAI API 타임아웃 (초)

# OpenAI 클라이언트 생성 시 반드시 timeout 설정
from openai import OpenAI
import os

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key, timeout=LLM_TIMEOUT)  # ⚠️ timeout 필수!
```

**재시도 로직 (필수)**:
```python
# backend/core/llm_service.py
LLM_MAX_RETRIES = 3  # 최대 재시도 횟수

# 재시도 로직 (지수 백오프)
last_error = None
for attempt in range(LLM_MAX_RETRIES):
    try:
        response = client.chat.completions.create(...)
        # 성공 시 처리
        return result
    except Exception as e:
        last_error = e
        error_type = type(e).__name__
        
        # 마지막 시도가 아니면 재시도
        if attempt < LLM_MAX_RETRIES - 1:
            wait_time = 2**attempt  # 지수 백오프: 1초, 2초, 4초
            logger.warning(
                f"[WARNING] LLM call attempt {attempt + 1}/{LLM_MAX_RETRIES} failed: "
                f"{error_type}: {str(e)[:100]}, retrying in {wait_time}s..."
            )
            time.sleep(wait_time)
        else:
            # 모든 재시도 실패 시 fallback
            logger.error(
                f"[ERROR] LLM call failed after {LLM_MAX_RETRIES} attempts: "
                f"{error_type}: {str(e)[:200]}"
            )
            # Fallback 로직 (정규식 기반 등)
            return fallback_result
```

### 2. 예외 처리 규칙

**모든 예외를 catch하고 재시도**:
```python
# ✅ 좋은 예: 모든 예외 처리
try:
    response = client.chat.completions.create(...)
except Exception as e:  # 모든 예외 처리
    # 타임아웃, Rate limit, 네트워크 오류 등 모두 처리
    last_error = e
    # 재시도 로직

# ❌ 나쁜 예: 특정 예외만 처리
try:
    response = client.chat.completions.create(...)
except TimeoutError:  # 타임아웃만 처리 → 다른 오류는 실패
    pass
```

**Fallback 전략**:
```python
# 모든 재시도 실패 시 fallback
if attempt == LLM_MAX_RETRIES - 1:
    logger.info("[FALLBACK] Using regex-based event generation")
    # 정규식 기반 기본 처리
    return {
        "event_type": EventType.TURN,
        "summary": summarize_turn(turn)  # 정규식 기반 요약
    }
```

### 3. 캐싱 규칙 (개선 버전)

**⚠️ 중요**: 캐시 시스템은 `cache_manager.py` 패턴을 적용하여 Race Condition 방지 및 결정적 해시 함수를 사용합니다.

**결정적 해시 함수 사용 (SHA-256)**:
```python
# backend/core/cache.py
import hashlib

def _generate_text_hash(text: str, max_length: int = 2000) -> str:
    """
    텍스트의 결정적 해시 생성 (SHA-256)
    
    ⚠️ Python의 hash() 함수는 비결정적이므로 사용 금지
    - 프로세스마다 다른 값 반환 가능
    - 음수 값 발생 가능
    - 캐시 키 충돌 가능
    """
    text_content = text[:max_length]
    hash_obj = hashlib.sha256(text_content.encode('utf-8'))
    return hash_obj.hexdigest()[:16]  # 16자리만 사용 (충분함)

# backend/core/llm_service.py
def _generate_cache_key(turn: Turn) -> tuple[str, str]:
    """
    결정적 캐시 키 생성 (SHA-256 사용)
    
    Returns:
        (cache_key, text_hash) 튜플
    """
    text_content = turn.body[:2000]  # 충분한 길이로 확장
    text_hash = _generate_text_hash(text_content, max_length=2000)
    cache_key = f"llm_{text_hash}"
    return cache_key, text_hash
```

**원자적 파일 저장 (Race Condition 방지)**:
```python
# backend/core/cache.py
def save_cached_result(
    cache_key: str, 
    result: dict, 
    text_hash: Optional[str] = None,
    turn_index: Optional[int] = None
) -> None:
    """
    결과를 캐시에 저장 (임시 파일 + 원자적 이동)
    
    ⚠️ 중요: 직접 파일 쓰기는 Race Condition 발생 가능
    - 여러 스레드가 동시에 같은 파일을 쓰면 손상 가능
    - 해결: 임시 파일에 먼저 저장 후 원자적 이동
    """
    cache_file = CACHE_DIR / f"{cache_key}.json"
    
    try:
        # 결과 복사본 생성
        result_to_cache = result.copy()
        
        # 캐시 메타데이터 추가 (검증 및 디버깅용)
        cache_meta = {
            "cached_at": time.time(),
            "cache_key": cache_key,
        }
        
        if text_hash:
            cache_meta["text_hash"] = text_hash
        
        if turn_index is not None:
            cache_meta["turn_index"] = turn_index
        
        result_to_cache["_cache_meta"] = cache_meta
        
        # 임시 파일로 먼저 저장 (원자적 이동을 위해)
        temp_file = cache_file.with_suffix('.tmp')
        
        with open(temp_file, "w", encoding="utf-8") as f:
            json.dump(result_to_cache, f, ensure_ascii=False, indent=2)
        
        # 원자적 이동 (Windows/Linux 모두 지원)
        temp_file.replace(cache_file)
        
    except IOError:
        # 캐시 저장 실패 시 무시 (디스크 공간 부족 등)
        pass
```

**텍스트 해시 검증 (충돌 감지)**:
```python
# backend/core/cache.py
def get_cached_result(cache_key: str, text_hash: Optional[str] = None) -> Optional[dict]:
    """
    캐시된 결과 조회 (텍스트 해시 검증 포함)
    
    Args:
        cache_key: 캐시 키
        text_hash: 텍스트 해시 (검증용, 선택적)
    
    Returns:
        캐시된 결과 (dict) 또는 None
    """
    cache_file = CACHE_DIR / f"{cache_key}.json"
    
    if not cache_file.exists():
        return None
    
    try:
        with open(cache_file, "r", encoding="utf-8") as f:
            cached_data = json.load(f)
        
        # 캐시 메타데이터 추출 (검증용)
        cached_meta = cached_data.get("_cache_meta", {})
        
        # 텍스트 해시 검증 (제공된 경우)
        if text_hash:
            cached_text_hash = cached_meta.get("text_hash")
            if cached_text_hash != text_hash:
                # 캐시 키 충돌 감지
                return None
        
        # 캐시 메타데이터 제거 (사용자에게 반환하지 않음)
        cached_data.pop("_cache_meta", None)
        
        return cached_data
        
    except (json.JSONDecodeError, IOError, KeyError):
        # 캐시 파일 손상 시 무시
        return None
```

**Fallback 시에도 캐시 저장**:
```python
# backend/core/llm_service.py
else:
    # 모든 재시도 실패 시 Fallback (캐시 저장 포함)
    logger.info(f"[FALLBACK] Using regex-based event generation for Turn {turn.turn_index}")
    
    summary = summarize_turn(turn)
    result = {
        "event_type": EventType.TURN.value,
        "summary": summary,
    }
    
    # ✅ Fallback 결과도 캐시 저장 (중요!)
    save_cached_result(
        cache_key, 
        result, 
        text_hash=text_hash,
        turn_index=turn.turn_index
    )
    with _cache_stats_lock:
        _cache_stats["saves"] += 1
    logger.info(f"[CACHE SAVE] Turn {turn.turn_index}: cache_key={cache_key} (fallback)")
    
    return {
        "event_type": EventType.TURN,
        "summary": summary,
    }
```

**캐시 통계 추적 (개선 버전)**:
```python
# Thread-safe 캐시 통계
_cache_stats = {"hits": 0, "misses": 0, "saves": 0}
_cache_stats_lock = threading.Lock()

def get_cache_stats() -> dict:
    """
    현재 캐시 통계 반환 (디렉토리 통계 포함)
    """
    with _cache_stats_lock:
        stats = _cache_stats.copy()
        
        # 디렉토리 통계 추가
        from backend.core.cache import get_cache_stats as get_cache_dir_stats
        dir_stats = get_cache_dir_stats()
        stats.update(dir_stats)
        
        # 히트율 계산
        total_requests = stats["hits"] + stats["misses"]
        if total_requests > 0:
            stats["hit_rate"] = stats["hits"] / total_requests
        else:
            stats["hit_rate"] = 0.0
        
        return stats

# 캐시 히트 시
with _cache_stats_lock:
    _cache_stats["hits"] += 1

# 캐시 미스 시
with _cache_stats_lock:
    _cache_stats["misses"] += 1
```

### 4. 병렬 처리 시 주의사항

**병렬 처리와 타임아웃**:
```python
# backend/builders/event_normalizer.py
from concurrent.futures import ThreadPoolExecutor, as_completed

def _normalize_turns_to_events_parallel(turns, session_meta):
    """
    LLM 기반 이벤트 생성 (병렬 처리)
    
    ⚠️ 주의: 각 스레드가 독립적인 OpenAI 클라이언트를 생성하므로
    타임아웃 설정이 각 스레드에 적용되어야 함
    """
    events_dict = {}
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        # 각 Turn을 병렬로 처리
        future_to_turn = {
            executor.submit(create_event_with_llm, turn, session_meta): turn
            for turn in turns
        }
        
        # 완료된 작업부터 처리
        for future in as_completed(future_to_turn):
            turn = future_to_turn[future]
            try:
                event = future.result()
                events_dict[turn.turn_index] = event
            except Exception as e:
                # 에러 발생 시 기본 이벤트 생성 (fallback)
                event = create_single_event_with_priority(turn, session_meta)
                events_dict[turn.turn_index] = event
    
    # turn_index 순서로 정렬
    events = [events_dict[turn.turn_index] for turn in turns]
    return events
```

**병렬 처리 시 동시 연결 제한**:
- `max_workers=5`로 설정 (OpenAI API 동시 연결 제한 고려)
- 동시 연결이 많아지면 connection pool 고갈 가능
- 필요 시 `max_workers`를 3 이하로 낮추거나 semaphore로 제한

## Implementation Patterns

### Pattern 1: 완전한 LLM 호출 함수 (개선 버전)

```python
# backend/core/llm_service.py
def classify_and_summarize_with_llm(turn: Turn) -> Dict[str, any]:
    """
    LLM으로 타입 분류 및 요약 생성 (개선 버전)
    
    개선 사항:
    1. 결정적 해시 함수 사용 (SHA-256)
    2. 텍스트 해시 검증으로 충돌 감지
    3. Fallback 시에도 캐시 저장
    4. 캐시 메타데이터 추가
    """
    # 1. 결정적 캐시 키 생성 (SHA-256 사용)
    text_content = turn.body[:2000]  # 충분한 길이로 확장
    text_hash = _generate_text_hash(text_content, max_length=2000)
    cache_key = f"llm_{text_hash}"
    cache_file = CACHE_DIR / f"{cache_key}.json"
    
    logger.debug(
        f"[CACHE] Turn {turn.turn_index}: cache_key={cache_key}, "
        f"text_hash={text_hash}, file_exists={cache_file.exists()}"
    )
    
    # 2. 캐시 확인 (텍스트 해시 검증 포함)
    cached = get_cached_result(cache_key, text_hash=text_hash)
    if cached:
        with _cache_stats_lock:
            _cache_stats["hits"] += 1
        logger.info(f"[CACHE HIT] Turn {turn.turn_index}: cache_key={cache_key}")
        return {
            "event_type": EventType(cached["event_type"]),
            "summary": cached["summary"],
        }
    
    # 3. 캐시 미스
    with _cache_stats_lock:
        _cache_stats["misses"] += 1
    logger.info(f"[CACHE MISS] Turn {turn.turn_index}: cache_key={cache_key}, calling LLM")
    
    # 4. LLM 호출 (재시도 로직 포함)
    from openai import OpenAI
    
    api_key = os.getenv("OPENAI_API_KEY")
    client = OpenAI(api_key=api_key, timeout=LLM_TIMEOUT)
    
    # 재시도 로직 (지수 백오프)
    last_error = None
    for attempt in range(LLM_MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model="gpt-4.1-mini",  # 사용자 지시대로 정확히 사용
                messages=[...],
                max_tokens=150,
                temperature=0.3,
            )
            
            # 응답 파싱 및 검증
            result_text = response.choices[0].message.content.strip()
            # ... 파싱 로직 ...
            
            result = {
                "event_type": event_type.value,
                "summary": summary,
            }
            
            # 5. 캐시 저장 (텍스트 해시 포함)
            save_cached_result(
                cache_key, 
                result, 
                text_hash=text_hash,
                turn_index=turn.turn_index
            )
            with _cache_stats_lock:
                _cache_stats["saves"] += 1
            logger.info(f"[CACHE SAVE] Turn {turn.turn_index}: cache_key={cache_key}")
            
            return {
                "event_type": event_type,
                "summary": summary,
            }
        
        # 모든 예외 처리 (타임아웃, Rate limit, 일시적 오류 등)
        except Exception as e:
            last_error = e
            error_type = type(e).__name__
            
            # 마지막 시도가 아니면 재시도
            if attempt < LLM_MAX_RETRIES - 1:
                wait_time = 2**attempt  # 지수 백오프: 1초, 2초, 4초
                logger.warning(
                    f"[WARNING] LLM call attempt {attempt + 1}/{LLM_MAX_RETRIES} failed for Turn {turn.turn_index}: "
                    f"{error_type}: {str(e)[:100]}, retrying in {wait_time}s..."
                )
                time.sleep(wait_time)
            else:
                # 6. 모든 재시도 실패 시 Fallback (캐시 저장 포함)
                logger.error(
                    f"[ERROR] LLM call failed after {LLM_MAX_RETRIES} attempts for Turn {turn.turn_index}: "
                    f"{error_type}: {str(e)[:200]}"
                )
                logger.info(
                    f"[FALLBACK] Using regex-based event generation for Turn {turn.turn_index}"
                )
                
                summary = summarize_turn(turn)
                result = {
                    "event_type": EventType.TURN.value,
                    "summary": summary,
                }
                
                # ✅ Fallback 결과도 캐시 저장 (중요!)
                save_cached_result(
                    cache_key, 
                    result, 
                    text_hash=text_hash,
                    turn_index=turn.turn_index
                )
                with _cache_stats_lock:
                    _cache_stats["saves"] += 1
                logger.info(
                    f"[CACHE SAVE] Turn {turn.turn_index}: cache_key={cache_key} (fallback)"
                )
                
                return {
                    "event_type": EventType.TURN,
                    "summary": summary,
                }
    
    # 모든 재시도 실패 시 마지막 오류를 다시 발생시킴 (fallback 후에는 도달하지 않음)
    raise last_error
```

### Pattern 2: 환경 변수 로드

```python
# backend/core/llm_service.py
from dotenv import load_dotenv
import os

# .env 파일 자동 로드 (모듈 최상단)
load_dotenv()

# API 키 가져오기
api_key = os.getenv("OPENAI_API_KEY")
```

### Pattern 3: 로깅

```python
# backend/core/llm_service.py
import logging

logger = logging.getLogger(__name__)

# 캐시 관련 로그
logger.debug(f"[CACHE] Turn {turn.turn_index}: cache_key={cache_key}, file_exists={cache_file.exists()}")
logger.info(f"[CACHE HIT] Turn {turn.turn_index}: cache_key={cache_key}")
logger.info(f"[CACHE MISS] Turn {turn.turn_index}: cache_key={cache_key}, calling LLM")
logger.info(f"[CACHE SAVE] Turn {turn.turn_index}: cache_key={cache_key}")

# 재시도 관련 로그
logger.warning(f"[WARNING] LLM call attempt {attempt + 1}/{LLM_MAX_RETRIES} failed: ...")
logger.error(f"[ERROR] LLM call failed after {LLM_MAX_RETRIES} attempts: ...")
logger.info(f"[FALLBACK] Using regex-based event generation for Turn {turn.turn_index}")
```

## Examples

### 완전한 LLM 서비스 모듈 예시

```python
# backend/core/llm_service.py
"""
LLM 서비스 모듈

gpt-4.1-mini를 사용하여 이벤트 타입 분류 및 요약 생성
"""

import logging
import threading
import time
import os
from typing import Dict
from dotenv import load_dotenv

# .env 파일 자동 로드
load_dotenv()

from backend.core.models import Turn, EventType
from backend.core.cache import get_cached_result, save_cached_result, CACHE_DIR
from backend.builders.event_normalizer import summarize_turn

# 로거 설정
logger = logging.getLogger(__name__)

# 실행 중 캐시 통계 추적 (thread-safe)
_cache_stats = {"hits": 0, "misses": 0, "saves": 0}
_cache_stats_lock = threading.Lock()

# LLM 설정 상수
LLM_TIMEOUT = 120  # OpenAI API 타임아웃 (초)
LLM_MAX_RETRIES = 3  # 최대 재시도 횟수

def classify_and_summarize_with_llm(turn: Turn) -> Dict[str, any]:
    """
    LLM으로 타입 분류 및 요약 생성 (gpt-4.1-mini, 캐싱 적용, 재시도 로직 포함)
    """
    # 캐시 확인 → LLM 호출 (재시도) → 캐시 저장 → Fallback
    # ... (위 Pattern 1 참조)
```

## Checklist

### LLM 서비스 구현 시
- [ ] 타임아웃 설정 (`timeout=120` 또는 적절한 값)
- [ ] 재시도 로직 구현 (`max_retries=3`, 지수 백오프)
- [ ] 모든 예외 처리 (`except Exception as e`)
- [ ] Fallback 전략 구현 (정규식 기반 등)
- [ ] **결정적 해시 함수 사용** (SHA-256, `hash()` 사용 금지)
- [ ] **원자적 파일 저장** (임시 파일 + 원자적 이동)
- [ ] **텍스트 해시 검증** (캐시 키 충돌 감지)
- [ ] **Fallback 시에도 캐시 저장**
- [ ] 캐시 메타데이터 추가 (`_cache_meta`)
- [ ] 환경 변수 로드 (`load_dotenv()`)
- [ ] 로깅 구현 (캐시 히트/미스, 재시도, 에러)
- [ ] Thread-safe 캐시 통계 추적 (병렬 처리 시)
- [ ] 캐시 통계에 히트율 및 디렉토리 통계 포함

### 병렬 처리 시
- [ ] `max_workers` 제한 (3-5 권장)
- [ ] 각 스레드에서 타임아웃 설정 확인
- [ ] 에러 발생 시 fallback 처리
- [ ] 순서 유지 (turn_index 기준 정렬)

### 네트워크 오류 대응
- [ ] 타임아웃 설정으로 무한 대기 방지
- [ ] 재시도 로직으로 일시적 오류 처리
- [ ] 지수 백오프로 서버 부하 감소
- [ ] Fallback으로 완전 실패 방지

## Common Pitfalls

### ❌ 피해야 할 것

1. **타임아웃 미설정**:
```python
# ❌ 나쁜 예
client = OpenAI()  # timeout 설정 없음 → 무한 대기 가능
```

2. **재시도 로직 없음**:
```python
# ❌ 나쁜 예
try:
    response = client.chat.completions.create(...)
except Exception as e:
    raise  # 즉시 실패 → 일시적 오류도 실패로 처리
```

3. **특정 예외만 처리**:
```python
# ❌ 나쁜 예
try:
    response = client.chat.completions.create(...)
except TimeoutError:  # 타임아웃만 처리 → Rate limit 등은 실패
    pass
```

4. **캐싱 없음**:
```python
# ❌ 나쁜 예
# 캐시 확인 없이 매번 LLM 호출 → 비용 증가
response = client.chat.completions.create(...)
```

5. **비결정적 해시 함수 사용**:
```python
# ❌ 나쁜 예
cache_key = f"llm_{hash(turn.body[:1000]) % 1000000}"
# 문제:
# - 프로세스마다 다른 값 반환 가능
# - 음수 값 발생 가능
# - 캐시 키 충돌 가능
```

6. **직접 파일 쓰기 (Race Condition)**:
```python
# ❌ 나쁜 예
with open(cache_file, "w", encoding="utf-8") as f:
    json.dump(result, f)
# 문제: 여러 스레드가 동시에 같은 파일을 쓰면 손상 가능
```

7. **Fallback 시 캐시 저장 안 함**:
```python
# ❌ 나쁜 예
# Fallback 결과를 캐시에 저장하지 않음
# → 동일한 텍스트에 대해 반복 LLM 호출
return fallback_result
```

### ✅ 권장 사항

1. **타임아웃 설정**:
```python
# ✅ 좋은 예
client = OpenAI(api_key=api_key, timeout=LLM_TIMEOUT)
```

2. **재시도 로직**:
```python
# ✅ 좋은 예
for attempt in range(LLM_MAX_RETRIES):
    try:
        response = client.chat.completions.create(...)
        return result
    except Exception as e:
        if attempt < LLM_MAX_RETRIES - 1:
            wait_time = 2**attempt
            time.sleep(wait_time)
        else:
            return fallback_result
```

3. **모든 예외 처리**:
```python
# ✅ 좋은 예
except Exception as e:  # 모든 예외 처리
    # 타임아웃, Rate limit, 네트워크 오류 등 모두 처리
```

4. **결정적 해시 함수 사용**:
```python
# ✅ 좋은 예
import hashlib

def _generate_text_hash(text: str, max_length: int = 2000) -> str:
    text_content = text[:max_length]
    hash_obj = hashlib.sha256(text_content.encode('utf-8'))
    return hash_obj.hexdigest()[:16]

cache_key = f"llm_{_generate_text_hash(turn.body[:2000])}"
```

5. **원자적 파일 저장**:
```python
# ✅ 좋은 예
# 임시 파일로 먼저 저장 후 원자적 이동
temp_file = cache_file.with_suffix('.tmp')
with open(temp_file, "w", encoding="utf-8") as f:
    json.dump(result_to_cache, f, ensure_ascii=False, indent=2)
temp_file.replace(cache_file)  # 원자적 이동
```

6. **텍스트 해시 검증**:
```python
# ✅ 좋은 예
cached = get_cached_result(cache_key, text_hash=text_hash)
# 텍스트 해시 검증으로 캐시 키 충돌 감지
```

7. **Fallback 시에도 캐시 저장**:
```python
# ✅ 좋은 예
# Fallback 결과도 캐시 저장
save_cached_result(
    cache_key, 
    result, 
    text_hash=text_hash,
    turn_index=turn.turn_index
)
```

8. **캐싱 적용**:
```python
# ✅ 좋은 예
cached = get_cached_result(cache_key, text_hash=text_hash)
if cached:
    return cached_result
# 캐시 미스 시에만 LLM 호출
```

## 캐시 시스템 개선 사항

### 개선 전후 비교

| 항목 | 개선 전 | 개선 후 |
|------|---------|---------|
| 해시 함수 | `hash()` (비결정적) | SHA-256 (결정적) |
| 파일 저장 | 직접 쓰기 | 임시 파일 + 원자적 이동 |
| Race Condition | 가능 | 원자적 이동으로 방지 |
| 캐시 검증 | 없음 | 텍스트 해시 검증 |
| Fallback 캐시 | 저장 안 함 | 저장함 |
| 캐시 메타데이터 | 없음 | 추가 (검증/디버깅) |
| 통계 기능 | 기본 | 디렉토리 통계 포함 |

### 검증된 성능 개선

**테스트 결과** (67개 Turn 처리):
- 첫 실행 (캐시 없음): 56.01초, 캐시 히트율 0%
- 두 번째 실행 (캐시 있음): 0.26초, 캐시 히트율 100%
- **성능 향상**: 약 215배 (56초 → 0.26초)
- **비용 절감**: LLM 호출 100% 감소 (두 번째 실행 기준)

### 캐시 시스템 핵심 원칙

1. **결정적 해시**: SHA-256 사용으로 프로세스 재시작 후에도 동일한 캐시 키 생성
2. **원자적 저장**: 임시 파일 + 원자적 이동으로 Race Condition 방지
3. **충돌 감지**: 텍스트 해시 검증으로 잘못된 캐시 결과 반환 방지
4. **Fallback 캐시**: 재시도 실패 시에도 결과 저장하여 재사용 가능
5. **메타데이터**: 검증 및 디버깅을 위한 캐시 메타데이터 저장

## References
- LLM 서비스 구현: `backend/core/llm_service.py`
- 캐시 관리: `backend/core/cache.py`
- 캐시 매니저 참고: `docs/cache_manager.py` (원자적 저장 패턴)
- 이벤트 정규화: `backend/builders/event_normalizer.py`
- 테스트 전략: `.cursor/rules/testing-strategy.mdc`
